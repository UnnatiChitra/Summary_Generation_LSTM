{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UnnatiChitra/Summary_Generation_LSTM/blob/main/Summary_Generation_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950e6b19",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "950e6b19",
        "outputId": "18465ca7-9704-49ae-ed33-8da356c074ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.22.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.65.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=8fcb4ea0db516c16a8d93f3ac47b9beebc48a824c446ad85b22e2e81f4e3d581\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a03dd074",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a03dd074"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8b7e9c",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8c8b7e9c"
      },
      "outputs": [],
      "source": [
        "# Setup some configuration parameters\n",
        "\n",
        "config = {'min_text_len':40,\n",
        "          'max_text_len':60,\n",
        "          'max_summary_len':30,\n",
        "          'latent_dim' : 300,\n",
        "          'embedding_dim' : 200}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a23346b",
      "metadata": {
        "id": "3a23346b"
      },
      "source": [
        "---\n",
        "# Reading the dataset\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lW3lHyErPRIR",
      "metadata": {
        "id": "lW3lHyErPRIR"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cEpQW_mGxtT",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4cEpQW_mGxtT"
      },
      "outputs": [],
      "source": [
        "!gdown 1sui9RXxVsPDa4s2kooQwRGhb8taZhcgD\n",
        "!gdown 1H3gdo7SLBiWE_GGD6_xcdAp2wJJFcd5L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3215e0d",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e3215e0d",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "summary = pd.read_csv('news_summary.csv', encoding='iso-8859-1')\n",
        "raw = pd.read_csv('news_summary_more.csv', encoding='iso-8859-1')\n",
        "\n",
        "raw = raw.rename(columns = {'headlines':'summary'})\n",
        "summary = summary[['headlines', 'text']].rename(columns={'headlines':'summary'})\n",
        "\n",
        "# Concatenate the summary and the raw files\n",
        "df = pd.concat([raw, summary]).reset_index(drop=True)\n",
        "\n",
        "summary.shape, raw.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ca4bef4",
      "metadata": {
        "id": "4ca4bef4"
      },
      "source": [
        "### Distribution of words before filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7663f538",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7663f538"
      },
      "outputs": [],
      "source": [
        "fig, ax =plt.subplots(1,2, figsize=(20,2.5))\n",
        "sns.boxplot(raw.text.str.split().str.len(), ax=ax[0])\n",
        "ax[0].set_title('text')\n",
        "sns.boxplot(raw.summary.str.split().str.len(), ax=ax[1])\n",
        "ax[1].set_title('summary')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c78111af",
      "metadata": {
        "id": "c78111af"
      },
      "source": [
        "### Filter text in the range between 40 and 60 words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c717dca",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3c717dca"
      },
      "outputs": [],
      "source": [
        "print(f'Before filtering: {raw.shape}')\n",
        "pre = df.loc[((df['text'].str.split(\" \").str.len()>config['min_text_len'])\n",
        "               &(df['text'].str.split(\" \").str.len()<config['max_text_len']))].reset_index(drop=True)\n",
        "print(f'After filtering: {pre.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40a89005",
      "metadata": {
        "id": "40a89005"
      },
      "source": [
        "### Distribution of words after filtering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b457e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "83b457e7",
        "outputId": "eeb7619e-917c-46fa-f8d3-e3b97524e47c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1440x180 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax =plt.subplots(1,2, figsize=(20,2.5))\n",
        "sns.boxplot(pre.text.str.split().str.len(), ax=ax[0])\n",
        "ax[0].set_title('text')\n",
        "sns.boxplot(pre.summary.str.split().str.len(), ax=ax[1])\n",
        "ax[1].set_title('summary')\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c2b78e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1c2b78e4",
        "outputId": "c392cb95-c702-4773-ebcc-f7cb47d62586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: India recorded their lowest ODI total in New Zealand after getting all out for 92 runs in 30.5 overs in the fourth ODI at Hamilton on Thursday. Seven of India's batsmen were dismissed for single-digit scores, while their number ten batsman Yuzvendra Chahal top-scored with 18*(37). India's previous lowest ODI total in New Zealand was 108.\n",
            "\n",
            "Summary: India get all out for 92, their lowest ODI total in New Zealand\n",
            "\n",
            "Text length: 56\n",
            "Summary length: 13\n"
          ]
        }
      ],
      "source": [
        "ind = 1\n",
        "print(f'Text: {pre.text[ind]}')\n",
        "print()\n",
        "print(f'Summary: {pre.summary[ind]}')\n",
        "print()\n",
        "print(f'Text length: {len(pre.text[ind].split())}')\n",
        "print(f'Summary length: {len(pre.summary[ind].split())}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FDkctCvARuHS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDkctCvARuHS",
        "outputId": "2bb644f9-32ea-4cd0-ce60-6a79267e722b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ca1daa9",
      "metadata": {
        "id": "3ca1daa9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Embedding, Input, InputLayer, RNN, SimpleRNN, LSTM, Bidirectional, TimeDistributed\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import spacy\n",
        "from time import time\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4de3256",
      "metadata": {
        "id": "d4de3256"
      },
      "source": [
        "# Text preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e29b6e7e",
      "metadata": {
        "id": "e29b6e7e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Remove non-alphabetic characters (Data Cleaning)\n",
        "def text_strip(sentence):\n",
        "\n",
        "    sentence = re.sub(\"(\\\\t)\", \" \", str(sentence)).lower()\n",
        "    sentence = re.sub(\"(\\\\r)\", \" \", str(sentence)).lower()\n",
        "    sentence = re.sub(\"(\\\\n)\", \" \", str(sentence)).lower()\n",
        "\n",
        "    # Remove - if it occurs more than one time consecutively\n",
        "    sentence = re.sub(\"(--+)\", \" \", str(sentence)).lower()\n",
        "\n",
        "    # Remove . if it occurs more than one time consecutively\n",
        "    sentence = re.sub(\"(\\.\\.+)\", \" \", str(sentence)).lower()\n",
        "\n",
        "    # Remove the characters - <>()|&©ø\"',;?~*!\n",
        "    sentence = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", \" \", str(sentence)).lower()\n",
        "\n",
        "    # Remove \\x9* in text\n",
        "    sentence = re.sub(r\"(\\\\x9\\d)\", \" \", str(sentence)).lower()\n",
        "\n",
        "    # Replace CM# and CHG# to CM_NUM\n",
        "    sentence = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(sentence)).lower()\n",
        "\n",
        "    # Remove punctuations at the end of a word\n",
        "    sentence = re.sub(\"(\\.\\s+)\", \" \", str(sentence)).lower()\n",
        "    sentence = re.sub(\"(\\-\\s+)\", \" \", str(sentence)).lower()\n",
        "    sentence = re.sub(\"(\\:\\s+)\", \" \", str(sentence)).lower()\n",
        "\n",
        "    # Remove multiple spaces\n",
        "    sentence = re.sub(\"(\\s+)\", \" \", str(sentence)).lower()\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb5ce8e2",
      "metadata": {
        "id": "fb5ce8e2"
      },
      "source": [
        "# Get the cleaned text and Add start, end tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1e0108",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "9a1e0108",
        "outputId": "8402bda3-8908-4642-a6c9-44c143239efa",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-89fe2384-15cb-477b-900f-04d204f03f73\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>summary</th>\n",
              "      <th>text</th>\n",
              "      <th>cleaned_text</th>\n",
              "      <th>cleaned_summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Rahat Fateh Ali Khan denies getting notice for...</td>\n",
              "      <td>Pakistani singer Rahat Fateh Ali Khan has deni...</td>\n",
              "      <td>pakistani singer rahat fateh ali khan has deni...</td>\n",
              "      <td>sostok _START_ rahat fateh ali khan denies get...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>India get all out for 92, their lowest ODI tot...</td>\n",
              "      <td>India recorded their lowest ODI total in New Z...</td>\n",
              "      <td>india recorded their lowest odi total in new z...</td>\n",
              "      <td>sostok _START_ india get all out for 92 their ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Called PM Modi 'sir' 10 times to satisfy his e...</td>\n",
              "      <td>Andhra Pradesh CM N Chandrababu Naidu has said...</td>\n",
              "      <td>andhra pradesh cm n chandrababu naidu has said...</td>\n",
              "      <td>sostok _START_ called pm modi sir 10 times to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81-yr-old woman conducts physical training in ...</td>\n",
              "      <td>Isha Ghosh, an 81-year-old member of Bharat Sc...</td>\n",
              "      <td>isha ghosh an 81-year-old member of bharat sco...</td>\n",
              "      <td>sostok _START_ 81-yr-old woman conducts physic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Karan Johar, Tabu turn showstoppers on opening...</td>\n",
              "      <td>Filmmaker Karan Johar and actress Tabu turned ...</td>\n",
              "      <td>filmmaker karan johar and actress tabu turned ...</td>\n",
              "      <td>sostok _START_ karan johar tabu turn showstopp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54567</th>\n",
              "      <td>Prosthetic fingers on sale to rig UP election:...</td>\n",
              "      <td>An investigation by India Today has unmasked a...</td>\n",
              "      <td>an investigation by india today has unmasked a...</td>\n",
              "      <td>sostok _START_ prosthetic fingers on sale to r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54568</th>\n",
              "      <td>Kangana, Shahid, Saif starrer 'Rangoon' hits t...</td>\n",
              "      <td>The Kangana Ranaut, Shahid Kapoor and Saif Ali...</td>\n",
              "      <td>the kangana ranaut shahid kapoor and saif ali ...</td>\n",
              "      <td>sostok _START_ kangana shahid saif starrer ran...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54569</th>\n",
              "      <td>Ticket collector bites off senior officer?s no...</td>\n",
              "      <td>A ticket collector on Thursday allegedly bit o...</td>\n",
              "      <td>a ticket collector on thursday allegedly bit o...</td>\n",
              "      <td>sostok _START_ ticket collector bites off seni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54570</th>\n",
              "      <td>Shouldn't rob their childhood: Aamir on kids r...</td>\n",
              "      <td>Aamir Khan, while talking about reality shows ...</td>\n",
              "      <td>aamir khan while talking about reality shows o...</td>\n",
              "      <td>sostok _START_ shouldn t rob their childhood a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54571</th>\n",
              "      <td>Asha Bhosle gets ?53,000 power bill for unused...</td>\n",
              "      <td>The Maharashtra government has initiated an in...</td>\n",
              "      <td>the maharashtra government has initiated an in...</td>\n",
              "      <td>sostok _START_ asha bhosle gets 53 000 power b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>54572 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89fe2384-15cb-477b-900f-04d204f03f73')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89fe2384-15cb-477b-900f-04d204f03f73 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89fe2384-15cb-477b-900f-04d204f03f73');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                 summary  \\\n",
              "0      Rahat Fateh Ali Khan denies getting notice for...   \n",
              "1      India get all out for 92, their lowest ODI tot...   \n",
              "2      Called PM Modi 'sir' 10 times to satisfy his e...   \n",
              "3      81-yr-old woman conducts physical training in ...   \n",
              "4      Karan Johar, Tabu turn showstoppers on opening...   \n",
              "...                                                  ...   \n",
              "54567  Prosthetic fingers on sale to rig UP election:...   \n",
              "54568  Kangana, Shahid, Saif starrer 'Rangoon' hits t...   \n",
              "54569  Ticket collector bites off senior officer?s no...   \n",
              "54570  Shouldn't rob their childhood: Aamir on kids r...   \n",
              "54571  Asha Bhosle gets ?53,000 power bill for unused...   \n",
              "\n",
              "                                                    text  \\\n",
              "0      Pakistani singer Rahat Fateh Ali Khan has deni...   \n",
              "1      India recorded their lowest ODI total in New Z...   \n",
              "2      Andhra Pradesh CM N Chandrababu Naidu has said...   \n",
              "3      Isha Ghosh, an 81-year-old member of Bharat Sc...   \n",
              "4      Filmmaker Karan Johar and actress Tabu turned ...   \n",
              "...                                                  ...   \n",
              "54567  An investigation by India Today has unmasked a...   \n",
              "54568  The Kangana Ranaut, Shahid Kapoor and Saif Ali...   \n",
              "54569  A ticket collector on Thursday allegedly bit o...   \n",
              "54570  Aamir Khan, while talking about reality shows ...   \n",
              "54571  The Maharashtra government has initiated an in...   \n",
              "\n",
              "                                            cleaned_text  \\\n",
              "0      pakistani singer rahat fateh ali khan has deni...   \n",
              "1      india recorded their lowest odi total in new z...   \n",
              "2      andhra pradesh cm n chandrababu naidu has said...   \n",
              "3      isha ghosh an 81-year-old member of bharat sco...   \n",
              "4      filmmaker karan johar and actress tabu turned ...   \n",
              "...                                                  ...   \n",
              "54567  an investigation by india today has unmasked a...   \n",
              "54568  the kangana ranaut shahid kapoor and saif ali ...   \n",
              "54569  a ticket collector on thursday allegedly bit o...   \n",
              "54570  aamir khan while talking about reality shows o...   \n",
              "54571  the maharashtra government has initiated an in...   \n",
              "\n",
              "                                         cleaned_summary  \n",
              "0      sostok _START_ rahat fateh ali khan denies get...  \n",
              "1      sostok _START_ india get all out for 92 their ...  \n",
              "2      sostok _START_ called pm modi sir 10 times to ...  \n",
              "3      sostok _START_ 81-yr-old woman conducts physic...  \n",
              "4      sostok _START_ karan johar tabu turn showstopp...  \n",
              "...                                                  ...  \n",
              "54567  sostok _START_ prosthetic fingers on sale to r...  \n",
              "54568  sostok _START_ kangana shahid saif starrer ran...  \n",
              "54569  sostok _START_ ticket collector bites off seni...  \n",
              "54570  sostok _START_ shouldn t rob their childhood a...  \n",
              "54571  sostok _START_ asha bhosle gets 53 000 power b...  \n",
              "\n",
              "[54572 rows x 4 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre['cleaned_text'] = pre.text.apply(lambda x: text_strip(x))\n",
        "pre['cleaned_summary'] = pre.summary.apply(lambda x: '_START_ '+ text_strip(x) + ' _END_')\n",
        "pre['cleaned_summary'] = pre['cleaned_summary'].apply(lambda x: 'sostok ' + x + ' eostok')\n",
        "pre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97606ca5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "97606ca5",
        "outputId": "9809ef21-929b-4166-e55d-710de6045ebe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(54572, 4)\n",
            "(47500, 4)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9afd6f60-fa17-4202-bb06-648ad1497d60\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>pakistani singer rahat fateh ali khan has deni...</td>\n",
              "      <td>sostok _START_ rahat fateh ali khan denies get...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>india recorded their lowest odi total in new z...</td>\n",
              "      <td>sostok _START_ india get all out for 92 their ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>andhra pradesh cm n chandrababu naidu has said...</td>\n",
              "      <td>sostok _START_ called pm modi sir 10 times to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>isha ghosh an 81-year-old member of bharat sco...</td>\n",
              "      <td>sostok _START_ 81-yr-old woman conducts physic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>filmmaker karan johar and actress tabu turned ...</td>\n",
              "      <td>sostok _START_ karan johar tabu turn showstopp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47495</th>\n",
              "      <td>indian captain virat kohli on friday got out w...</td>\n",
              "      <td>sostok _START_ virat kohli out for a duck for ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47496</th>\n",
              "      <td>srinivas kunchubhotla 32 an indian engineer wa...</td>\n",
              "      <td>sostok _START_ indian shot dead in us over all...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47497</th>\n",
              "      <td>an investigation by india today has unmasked a...</td>\n",
              "      <td>sostok _START_ prosthetic fingers on sale to r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47498</th>\n",
              "      <td>a ticket collector on thursday allegedly bit o...</td>\n",
              "      <td>sostok _START_ ticket collector bites off seni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47499</th>\n",
              "      <td>the maharashtra government has initiated an in...</td>\n",
              "      <td>sostok _START_ asha bhosle gets 53 000 power b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>47500 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9afd6f60-fa17-4202-bb06-648ad1497d60')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9afd6f60-fa17-4202-bb06-648ad1497d60 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9afd6f60-fa17-4202-bb06-648ad1497d60');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    text  \\\n",
              "0      pakistani singer rahat fateh ali khan has deni...   \n",
              "1      india recorded their lowest odi total in new z...   \n",
              "2      andhra pradesh cm n chandrababu naidu has said...   \n",
              "3      isha ghosh an 81-year-old member of bharat sco...   \n",
              "4      filmmaker karan johar and actress tabu turned ...   \n",
              "...                                                  ...   \n",
              "47495  indian captain virat kohli on friday got out w...   \n",
              "47496  srinivas kunchubhotla 32 an indian engineer wa...   \n",
              "47497  an investigation by india today has unmasked a...   \n",
              "47498  a ticket collector on thursday allegedly bit o...   \n",
              "47499  the maharashtra government has initiated an in...   \n",
              "\n",
              "                                                 summary  \n",
              "0      sostok _START_ rahat fateh ali khan denies get...  \n",
              "1      sostok _START_ india get all out for 92 their ...  \n",
              "2      sostok _START_ called pm modi sir 10 times to ...  \n",
              "3      sostok _START_ 81-yr-old woman conducts physic...  \n",
              "4      sostok _START_ karan johar tabu turn showstopp...  \n",
              "...                                                  ...  \n",
              "47495  sostok _START_ virat kohli out for a duck for ...  \n",
              "47496  sostok _START_ indian shot dead in us over all...  \n",
              "47497  sostok _START_ prosthetic fingers on sale to r...  \n",
              "47498  sostok _START_ ticket collector bites off seni...  \n",
              "47499  sostok _START_ asha bhosle gets 53 000 power b...  \n",
              "\n",
              "[47500 rows x 2 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(pre.shape)\n",
        "post_pre = pre[((pre.cleaned_text.str.split().str.len()<=config['max_text_len']) &\n",
        "                (pre.summary.str.split().str.len()<=(config['max_summary_len']+4)))].copy()\n",
        "post_pre = post_pre.reset_index(drop=True)\n",
        "print(post_pre.shape)\n",
        "\n",
        "post_pre = post_pre.drop(['text', 'summary'], axis=1)\n",
        "post_pre = post_pre.rename(columns = {'cleaned_text':'text',\n",
        "                                      'cleaned_summary':'summary'})\n",
        "post_pre"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82798bef",
      "metadata": {
        "id": "82798bef"
      },
      "source": [
        "# Split the train and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e3986ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e3986ee",
        "outputId": "6b4dd4f9-7558-4017-b1ed-948568eb07b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((42750,), (4750,), (42750,), (4750,))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train, x_valid, y_train, y_valid = train_test_split(np.array(post_pre[\"text\"]),\n",
        "                                            np.array(post_pre[\"summary\"]),\n",
        "                                            test_size=0.1,\n",
        "                                            random_state=0,\n",
        "                                            shuffle=True\n",
        "                                           )\n",
        "\n",
        "x_train.shape, x_valid.shape, y_train.shape, y_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d354e9",
      "metadata": {
        "id": "63d354e9"
      },
      "outputs": [],
      "source": [
        "def get_rare_words(text_col):\n",
        "\n",
        "    # Prepare a tokenizer on testing data\n",
        "    text_tokenizer = Tokenizer()\n",
        "    text_tokenizer.fit_on_texts(list(text_col))\n",
        "\n",
        "    thresh = 5\n",
        "\n",
        "    cnt = 0\n",
        "    tot_cnt = 0\n",
        "\n",
        "    for key, value in text_tokenizer.word_counts.items():\n",
        "        tot_cnt = tot_cnt + 1\n",
        "        if value < thresh:\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
        "\n",
        "    return cnt, tot_cnt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "112491ec",
      "metadata": {
        "id": "112491ec"
      },
      "source": [
        "### Tokenize the train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e0d77b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50e0d77b",
        "outputId": "2f5c626b-f8c5-4f7f-eeca-b2990fabb035"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 64.22363847045192\n"
          ]
        }
      ],
      "source": [
        "x_train_cnt, x_train_tot_cnt = get_rare_words(text_col=x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f619772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f619772",
        "outputId": "cc53af90-ce5c-4c79-f1ad-4a13a2f1190d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary in X = 20996\n"
          ]
        }
      ],
      "source": [
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "x_tokenizer = Tokenizer(num_words=x_train_tot_cnt - x_train_cnt)\n",
        "# x_tokenizer = Tokenizer(num_words = x_train_tot_cnt)\n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "x_tr_seq = x_tokenizer.texts_to_sequences(x_train)\n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_valid)\n",
        "\n",
        "# Pad zero upto maximum length\n",
        "x_tr = pad_sequences(x_tr_seq,  maxlen=config['max_text_len'], padding='post')\n",
        "x_val = pad_sequences(x_val_seq, maxlen=config['max_text_len'], padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "x_voc = x_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in X = {}\".format(x_voc))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c582ec42",
      "metadata": {
        "id": "c582ec42"
      },
      "source": [
        "### Tokenize the validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc2593d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc2593d8",
        "outputId": "80de3fa1-39fe-4dde-9ddf-5d85e685f16c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 66.49736426949863\n"
          ]
        }
      ],
      "source": [
        "y_train_cnt, y_train_tot_cnt = get_rare_words(text_col=y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ecec403",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ecec403",
        "outputId": "7f1d30c1-2457-4e8b-b1ff-aecf11665657"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary in Y = 8708\n"
          ]
        }
      ],
      "source": [
        "# Prepare a tokenizer, again -- by not considering the rare words\n",
        "y_tokenizer = Tokenizer(num_words=y_train_tot_cnt - y_train_cnt)\n",
        "# y_tokenizer = Tokenizer(num_words=y_train_tot_cnt)\n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "# Convert text sequences to integer sequences\n",
        "y_tr_seq = y_tokenizer.texts_to_sequences(y_train)\n",
        "y_val_seq = y_tokenizer.texts_to_sequences(y_valid)\n",
        "\n",
        "# Pad zero upto maximum length\n",
        "y_tr = pad_sequences(y_tr_seq, maxlen=config['max_summary_len'], padding='post')\n",
        "y_val = pad_sequences(y_val_seq, maxlen=config['max_summary_len'], padding='post')\n",
        "\n",
        "# Size of vocabulary (+1 for padding token)\n",
        "y_voc = y_tokenizer.num_words + 1\n",
        "\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31525c29",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31525c29",
        "outputId": "1f3959d5-1d60-4e68-e8e5-510cd21136b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 60)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 60, 200)      4199200     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    [(None, 60, 300),    601200      ['embedding[0][0]']              \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  [(None, 60, 300),    721200      ['lstm[0][0]']                   \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, None, 200)    1741600     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  [(None, 60, 300),    721200      ['lstm_1[0][0]']                 \n",
            "                                 (None, 300),                                                     \n",
            "                                 (None, 300)]                                                     \n",
            "                                                                                                  \n",
            " lstm_3 (LSTM)                  [(None, None, 300),  601200      ['embedding_1[0][0]',            \n",
            "                                 (None, 300),                     'lstm_2[0][1]',                 \n",
            "                                 (None, 300)]                     'lstm_2[0][2]']                 \n",
            "                                                                                                  \n",
            " time_distributed (TimeDistribu  (None, None, 8708)  2621108     ['lstm_3[0][0]']                 \n",
            " ted)                                                                                             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 11,206,708\n",
            "Trainable params: 11,206,708\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "latent_dim = config['latent_dim']\n",
        "embedding_dim = config['embedding_dim']\n",
        "max_text_len = config['max_text_len']\n",
        "max_summary_len = config['max_summary_len']\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_text_len, ))\n",
        "\n",
        "# Embedding layer\n",
        "enc_emb = Embedding(x_voc, embedding_dim,\n",
        "                    trainable=True)(encoder_inputs)\n",
        "\n",
        "# Encoder LSTM 1\n",
        "encoder_lstm1 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output1, state_h1, state_c1) = encoder_lstm1(enc_emb)\n",
        "\n",
        "# Encoder LSTM 2\n",
        "encoder_lstm2 = LSTM(latent_dim, return_sequences=True,\n",
        "                     return_state=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_output2, state_h2, state_c2) = encoder_lstm2(encoder_output1)\n",
        "\n",
        "# Encoder LSTM 3\n",
        "encoder_lstm3 = LSTM(latent_dim, return_state=True,\n",
        "                     return_sequences=True, dropout=0.4,\n",
        "                     recurrent_dropout=0.4)\n",
        "(encoder_outputs, state_h, state_c) = encoder_lstm3(encoder_output2)\n",
        "\n",
        "# Set up the decoder, using encoder_states as the initial state\n",
        "decoder_inputs = Input(shape=(None, ))\n",
        "\n",
        "# Embedding layer\n",
        "dec_emb_layer = Embedding(y_voc, embedding_dim, trainable=True)\n",
        "dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# Decoder LSTM\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True,\n",
        "                    return_state=True, dropout=0.4,\n",
        "                    recurrent_dropout=0.2)\n",
        "(decoder_outputs, decoder_fwd_state, decoder_back_state) = \\\n",
        "    decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "# Dense layer\n",
        "decoder_dense = TimeDistributed(Dense(y_voc, activation='softmax'))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0771d6a0",
      "metadata": {
        "id": "0771d6a0"
      },
      "source": [
        "# Compile and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76b8500f",
      "metadata": {
        "id": "76b8500f"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model_name = \"./model.h5\"\n",
        "\n",
        "save_model = tf.keras.callbacks.ModelCheckpoint(filepath=model_name,\n",
        "                                                save_weights_only=True,\n",
        "                                                save_best_only=True,\n",
        "                                                verbose=1)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23ab4a68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23ab4a68",
        "outputId": "5c71f702-7409-49f6-9f2f-83f1f4fbef40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 4.5180 - accuracy: 0.5909\n",
            "Epoch 1: val_loss improved from inf to 2.78202, saving model to ./model.h5\n",
            "42/42 [==============================] - 71s 1s/step - loss: 4.5180 - accuracy: 0.5909 - val_loss: 2.7820 - val_accuracy: 0.6204\n",
            "Epoch 2/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7255 - accuracy: 0.6229\n",
            "Epoch 2: val_loss improved from 2.78202 to 2.66276, saving model to ./model.h5\n",
            "42/42 [==============================] - 68s 2s/step - loss: 2.7255 - accuracy: 0.6229 - val_loss: 2.6628 - val_accuracy: 0.6298\n",
            "Epoch 3/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.7010 - accuracy: 0.6477\n",
            "Epoch 3: val_loss improved from 2.66276 to 2.53363, saving model to ./model.h5\n",
            "42/42 [==============================] - 60s 1s/step - loss: 2.7010 - accuracy: 0.6477 - val_loss: 2.5336 - val_accuracy: 0.6820\n",
            "Epoch 4/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.5031 - accuracy: 0.6858\n",
            "Epoch 4: val_loss improved from 2.53363 to 2.41976, saving model to ./model.h5\n",
            "42/42 [==============================] - 59s 1s/step - loss: 2.5031 - accuracy: 0.6858 - val_loss: 2.4198 - val_accuracy: 0.6937\n",
            "Epoch 5/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.4325 - accuracy: 0.6914\n",
            "Epoch 5: val_loss improved from 2.41976 to 2.38162, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.4325 - accuracy: 0.6914 - val_loss: 2.3816 - val_accuracy: 0.6967\n",
            "Epoch 6/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.3945 - accuracy: 0.6930\n",
            "Epoch 6: val_loss improved from 2.38162 to 2.34645, saving model to ./model.h5\n",
            "42/42 [==============================] - 59s 1s/step - loss: 2.3945 - accuracy: 0.6930 - val_loss: 2.3464 - val_accuracy: 0.6968\n",
            "Epoch 7/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.3579 - accuracy: 0.6936\n",
            "Epoch 7: val_loss improved from 2.34645 to 2.32086, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.3579 - accuracy: 0.6936 - val_loss: 2.3209 - val_accuracy: 0.6977\n",
            "Epoch 8/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.3357 - accuracy: 0.6944\n",
            "Epoch 8: val_loss improved from 2.32086 to 2.30424, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.3357 - accuracy: 0.6944 - val_loss: 2.3042 - val_accuracy: 0.6979\n",
            "Epoch 9/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.3166 - accuracy: 0.6950\n",
            "Epoch 9: val_loss improved from 2.30424 to 2.28904, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.3166 - accuracy: 0.6950 - val_loss: 2.2890 - val_accuracy: 0.6988\n",
            "Epoch 10/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2978 - accuracy: 0.6959\n",
            "Epoch 10: val_loss improved from 2.28904 to 2.27266, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.2978 - accuracy: 0.6959 - val_loss: 2.2727 - val_accuracy: 0.6994\n",
            "Epoch 11/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2790 - accuracy: 0.6967\n",
            "Epoch 11: val_loss improved from 2.27266 to 2.25666, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.2790 - accuracy: 0.6967 - val_loss: 2.2567 - val_accuracy: 0.7003\n",
            "Epoch 12/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2593 - accuracy: 0.6977\n",
            "Epoch 12: val_loss improved from 2.25666 to 2.23950, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 2.2593 - accuracy: 0.6977 - val_loss: 2.2395 - val_accuracy: 0.7011\n",
            "Epoch 13/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2387 - accuracy: 0.6987\n",
            "Epoch 13: val_loss improved from 2.23950 to 2.22141, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.2387 - accuracy: 0.6987 - val_loss: 2.2214 - val_accuracy: 0.7020\n",
            "Epoch 14/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2166 - accuracy: 0.6998\n",
            "Epoch 14: val_loss improved from 2.22141 to 2.20156, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 2.2166 - accuracy: 0.6998 - val_loss: 2.2016 - val_accuracy: 0.7035\n",
            "Epoch 15/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1928 - accuracy: 0.7012\n",
            "Epoch 15: val_loss improved from 2.20156 to 2.18058, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 2.1928 - accuracy: 0.7012 - val_loss: 2.1806 - val_accuracy: 0.7049\n",
            "Epoch 16/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1668 - accuracy: 0.7027\n",
            "Epoch 16: val_loss improved from 2.18058 to 2.15707, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 2.1668 - accuracy: 0.7027 - val_loss: 2.1571 - val_accuracy: 0.7057\n",
            "Epoch 17/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1379 - accuracy: 0.7044\n",
            "Epoch 17: val_loss improved from 2.15707 to 2.13293, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 2.1379 - accuracy: 0.7044 - val_loss: 2.1329 - val_accuracy: 0.7073\n",
            "Epoch 18/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.1100 - accuracy: 0.7058\n",
            "Epoch 18: val_loss improved from 2.13293 to 2.11058, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 2.1100 - accuracy: 0.7058 - val_loss: 2.1106 - val_accuracy: 0.7086\n",
            "Epoch 19/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0836 - accuracy: 0.7071\n",
            "Epoch 19: val_loss improved from 2.11058 to 2.08970, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.0836 - accuracy: 0.7071 - val_loss: 2.0897 - val_accuracy: 0.7102\n",
            "Epoch 20/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0590 - accuracy: 0.7085\n",
            "Epoch 20: val_loss improved from 2.08970 to 2.07226, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.0590 - accuracy: 0.7085 - val_loss: 2.0723 - val_accuracy: 0.7110\n",
            "Epoch 21/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0367 - accuracy: 0.7099\n",
            "Epoch 21: val_loss improved from 2.07226 to 2.05497, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 2.0367 - accuracy: 0.7099 - val_loss: 2.0550 - val_accuracy: 0.7115\n",
            "Epoch 22/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.0156 - accuracy: 0.7111\n",
            "Epoch 22: val_loss improved from 2.05497 to 2.04007, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 2.0156 - accuracy: 0.7111 - val_loss: 2.0401 - val_accuracy: 0.7126\n",
            "Epoch 23/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9959 - accuracy: 0.7121\n",
            "Epoch 23: val_loss improved from 2.04007 to 2.02655, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.9959 - accuracy: 0.7121 - val_loss: 2.0265 - val_accuracy: 0.7139\n",
            "Epoch 24/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9772 - accuracy: 0.7134\n",
            "Epoch 24: val_loss improved from 2.02655 to 2.01318, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.9772 - accuracy: 0.7134 - val_loss: 2.0132 - val_accuracy: 0.7148\n",
            "Epoch 25/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9585 - accuracy: 0.7144\n",
            "Epoch 25: val_loss improved from 2.01318 to 2.00200, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.9585 - accuracy: 0.7144 - val_loss: 2.0020 - val_accuracy: 0.7153\n",
            "Epoch 26/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9415 - accuracy: 0.7154\n",
            "Epoch 26: val_loss improved from 2.00200 to 1.99055, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.9415 - accuracy: 0.7154 - val_loss: 1.9905 - val_accuracy: 0.7159\n",
            "Epoch 27/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9249 - accuracy: 0.7164\n",
            "Epoch 27: val_loss improved from 1.99055 to 1.97932, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.9249 - accuracy: 0.7164 - val_loss: 1.9793 - val_accuracy: 0.7168\n",
            "Epoch 28/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.9075 - accuracy: 0.7175\n",
            "Epoch 28: val_loss improved from 1.97932 to 1.96671, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.9075 - accuracy: 0.7175 - val_loss: 1.9667 - val_accuracy: 0.7175\n",
            "Epoch 29/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8883 - accuracy: 0.7187\n",
            "Epoch 29: val_loss improved from 1.96671 to 1.95535, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.8883 - accuracy: 0.7187 - val_loss: 1.9554 - val_accuracy: 0.7182\n",
            "Epoch 30/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8686 - accuracy: 0.7198\n",
            "Epoch 30: val_loss improved from 1.95535 to 1.93514, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.8686 - accuracy: 0.7198 - val_loss: 1.9351 - val_accuracy: 0.7199\n",
            "Epoch 31/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8482 - accuracy: 0.7211\n",
            "Epoch 31: val_loss improved from 1.93514 to 1.92243, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.8482 - accuracy: 0.7211 - val_loss: 1.9224 - val_accuracy: 0.7206\n",
            "Epoch 32/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8277 - accuracy: 0.7222\n",
            "Epoch 32: val_loss improved from 1.92243 to 1.90678, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.8277 - accuracy: 0.7222 - val_loss: 1.9068 - val_accuracy: 0.7216\n",
            "Epoch 33/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.8062 - accuracy: 0.7235\n",
            "Epoch 33: val_loss improved from 1.90678 to 1.89281, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.8062 - accuracy: 0.7235 - val_loss: 1.8928 - val_accuracy: 0.7224\n",
            "Epoch 34/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7845 - accuracy: 0.7249\n",
            "Epoch 34: val_loss improved from 1.89281 to 1.87653, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.7845 - accuracy: 0.7249 - val_loss: 1.8765 - val_accuracy: 0.7234\n",
            "Epoch 35/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7639 - accuracy: 0.7262\n",
            "Epoch 35: val_loss improved from 1.87653 to 1.86307, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.7639 - accuracy: 0.7262 - val_loss: 1.8631 - val_accuracy: 0.7245\n",
            "Epoch 36/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7442 - accuracy: 0.7274\n",
            "Epoch 36: val_loss improved from 1.86307 to 1.85265, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.7442 - accuracy: 0.7274 - val_loss: 1.8527 - val_accuracy: 0.7251\n",
            "Epoch 37/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7247 - accuracy: 0.7287\n",
            "Epoch 37: val_loss improved from 1.85265 to 1.83997, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.7247 - accuracy: 0.7287 - val_loss: 1.8400 - val_accuracy: 0.7264\n",
            "Epoch 38/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.7065 - accuracy: 0.7298\n",
            "Epoch 38: val_loss improved from 1.83997 to 1.82879, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.7065 - accuracy: 0.7298 - val_loss: 1.8288 - val_accuracy: 0.7268\n",
            "Epoch 39/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.6883 - accuracy: 0.7312\n",
            "Epoch 39: val_loss improved from 1.82879 to 1.81931, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.6883 - accuracy: 0.7312 - val_loss: 1.8193 - val_accuracy: 0.7274\n",
            "Epoch 40/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.6697 - accuracy: 0.7325\n",
            "Epoch 40: val_loss improved from 1.81931 to 1.80966, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.6697 - accuracy: 0.7325 - val_loss: 1.8097 - val_accuracy: 0.7279\n",
            "Epoch 41/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.6524 - accuracy: 0.7336\n",
            "Epoch 41: val_loss improved from 1.80966 to 1.80004, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.6524 - accuracy: 0.7336 - val_loss: 1.8000 - val_accuracy: 0.7291\n",
            "Epoch 42/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.6357 - accuracy: 0.7349\n",
            "Epoch 42: val_loss improved from 1.80004 to 1.79222, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.6357 - accuracy: 0.7349 - val_loss: 1.7922 - val_accuracy: 0.7299\n",
            "Epoch 43/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.6192 - accuracy: 0.7360\n",
            "Epoch 43: val_loss improved from 1.79222 to 1.78364, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.6192 - accuracy: 0.7360 - val_loss: 1.7836 - val_accuracy: 0.7305\n",
            "Epoch 44/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.6035 - accuracy: 0.7370\n",
            "Epoch 44: val_loss improved from 1.78364 to 1.77716, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.6035 - accuracy: 0.7370 - val_loss: 1.7772 - val_accuracy: 0.7310\n",
            "Epoch 45/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.5868 - accuracy: 0.7382\n",
            "Epoch 45: val_loss improved from 1.77716 to 1.76995, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.5868 - accuracy: 0.7382 - val_loss: 1.7700 - val_accuracy: 0.7314\n",
            "Epoch 46/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.5707 - accuracy: 0.7394\n",
            "Epoch 46: val_loss improved from 1.76995 to 1.76303, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.5707 - accuracy: 0.7394 - val_loss: 1.7630 - val_accuracy: 0.7322\n",
            "Epoch 47/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.5555 - accuracy: 0.7407\n",
            "Epoch 47: val_loss improved from 1.76303 to 1.75822, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.5555 - accuracy: 0.7407 - val_loss: 1.7582 - val_accuracy: 0.7323\n",
            "Epoch 48/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.5408 - accuracy: 0.7414\n",
            "Epoch 48: val_loss improved from 1.75822 to 1.75119, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.5408 - accuracy: 0.7414 - val_loss: 1.7512 - val_accuracy: 0.7330\n",
            "Epoch 49/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.5255 - accuracy: 0.7427\n",
            "Epoch 49: val_loss improved from 1.75119 to 1.74766, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.5255 - accuracy: 0.7427 - val_loss: 1.7477 - val_accuracy: 0.7328\n",
            "Epoch 50/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.5107 - accuracy: 0.7439\n",
            "Epoch 50: val_loss improved from 1.74766 to 1.74090, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.5107 - accuracy: 0.7439 - val_loss: 1.7409 - val_accuracy: 0.7332\n",
            "Epoch 51/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.4963 - accuracy: 0.7451\n",
            "Epoch 51: val_loss improved from 1.74090 to 1.73516, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.4963 - accuracy: 0.7451 - val_loss: 1.7352 - val_accuracy: 0.7343\n",
            "Epoch 52/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.4810 - accuracy: 0.7464\n",
            "Epoch 52: val_loss improved from 1.73516 to 1.73032, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.4810 - accuracy: 0.7464 - val_loss: 1.7303 - val_accuracy: 0.7345\n",
            "Epoch 53/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.4667 - accuracy: 0.7475\n",
            "Epoch 53: val_loss improved from 1.73032 to 1.72561, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.4667 - accuracy: 0.7475 - val_loss: 1.7256 - val_accuracy: 0.7349\n",
            "Epoch 54/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.4521 - accuracy: 0.7487\n",
            "Epoch 54: val_loss improved from 1.72561 to 1.72159, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.4521 - accuracy: 0.7487 - val_loss: 1.7216 - val_accuracy: 0.7357\n",
            "Epoch 55/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.4383 - accuracy: 0.7496\n",
            "Epoch 55: val_loss improved from 1.72159 to 1.71678, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.4383 - accuracy: 0.7496 - val_loss: 1.7168 - val_accuracy: 0.7358\n",
            "Epoch 56/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.4245 - accuracy: 0.7509\n",
            "Epoch 56: val_loss improved from 1.71678 to 1.71355, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.4245 - accuracy: 0.7509 - val_loss: 1.7136 - val_accuracy: 0.7362\n",
            "Epoch 57/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.4105 - accuracy: 0.7521\n",
            "Epoch 57: val_loss improved from 1.71355 to 1.70959, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.4105 - accuracy: 0.7521 - val_loss: 1.7096 - val_accuracy: 0.7367\n",
            "Epoch 58/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.3970 - accuracy: 0.7533\n",
            "Epoch 58: val_loss improved from 1.70959 to 1.70652, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.3970 - accuracy: 0.7533 - val_loss: 1.7065 - val_accuracy: 0.7369\n",
            "Epoch 59/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.3832 - accuracy: 0.7547\n",
            "Epoch 59: val_loss improved from 1.70652 to 1.70327, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.3832 - accuracy: 0.7547 - val_loss: 1.7033 - val_accuracy: 0.7371\n",
            "Epoch 60/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.3703 - accuracy: 0.7557\n",
            "Epoch 60: val_loss improved from 1.70327 to 1.69978, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.3703 - accuracy: 0.7557 - val_loss: 1.6998 - val_accuracy: 0.7374\n",
            "Epoch 61/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.3571 - accuracy: 0.7568\n",
            "Epoch 61: val_loss improved from 1.69978 to 1.69687, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.3571 - accuracy: 0.7568 - val_loss: 1.6969 - val_accuracy: 0.7382\n",
            "Epoch 62/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.3442 - accuracy: 0.7581\n",
            "Epoch 62: val_loss improved from 1.69687 to 1.69611, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.3442 - accuracy: 0.7581 - val_loss: 1.6961 - val_accuracy: 0.7380\n",
            "Epoch 63/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.3312 - accuracy: 0.7594\n",
            "Epoch 63: val_loss improved from 1.69611 to 1.69238, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.3312 - accuracy: 0.7594 - val_loss: 1.6924 - val_accuracy: 0.7385\n",
            "Epoch 64/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.3187 - accuracy: 0.7606\n",
            "Epoch 64: val_loss improved from 1.69238 to 1.69189, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.3187 - accuracy: 0.7606 - val_loss: 1.6919 - val_accuracy: 0.7385\n",
            "Epoch 65/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.3061 - accuracy: 0.7618\n",
            "Epoch 65: val_loss improved from 1.69189 to 1.68801, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.3061 - accuracy: 0.7618 - val_loss: 1.6880 - val_accuracy: 0.7391\n",
            "Epoch 66/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.2946 - accuracy: 0.7631\n",
            "Epoch 66: val_loss improved from 1.68801 to 1.68656, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.2946 - accuracy: 0.7631 - val_loss: 1.6866 - val_accuracy: 0.7392\n",
            "Epoch 67/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.2812 - accuracy: 0.7643\n",
            "Epoch 67: val_loss improved from 1.68656 to 1.68417, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.2812 - accuracy: 0.7643 - val_loss: 1.6842 - val_accuracy: 0.7397\n",
            "Epoch 68/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.2696 - accuracy: 0.7657\n",
            "Epoch 68: val_loss improved from 1.68417 to 1.68413, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.2696 - accuracy: 0.7657 - val_loss: 1.6841 - val_accuracy: 0.7394\n",
            "Epoch 69/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.2571 - accuracy: 0.7668\n",
            "Epoch 69: val_loss improved from 1.68413 to 1.68295, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.2571 - accuracy: 0.7668 - val_loss: 1.6829 - val_accuracy: 0.7401\n",
            "Epoch 70/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.2457 - accuracy: 0.7679\n",
            "Epoch 70: val_loss improved from 1.68295 to 1.68294, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.2457 - accuracy: 0.7679 - val_loss: 1.6829 - val_accuracy: 0.7392\n",
            "Epoch 71/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.2329 - accuracy: 0.7695\n",
            "Epoch 71: val_loss improved from 1.68294 to 1.68149, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.2329 - accuracy: 0.7695 - val_loss: 1.6815 - val_accuracy: 0.7405\n",
            "Epoch 72/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.2218 - accuracy: 0.7706\n",
            "Epoch 72: val_loss improved from 1.68149 to 1.68030, saving model to ./model.h5\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.2218 - accuracy: 0.7706 - val_loss: 1.6803 - val_accuracy: 0.7411\n",
            "Epoch 73/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.2111 - accuracy: 0.7718\n",
            "Epoch 73: val_loss improved from 1.68030 to 1.67903, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.2111 - accuracy: 0.7718 - val_loss: 1.6790 - val_accuracy: 0.7408\n",
            "Epoch 74/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1993 - accuracy: 0.7734\n",
            "Epoch 74: val_loss did not improve from 1.67903\n",
            "42/42 [==============================] - 58s 1s/step - loss: 1.1993 - accuracy: 0.7734 - val_loss: 1.6809 - val_accuracy: 0.7405\n",
            "Epoch 75/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1881 - accuracy: 0.7745\n",
            "Epoch 75: val_loss improved from 1.67903 to 1.67894, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.1881 - accuracy: 0.7745 - val_loss: 1.6789 - val_accuracy: 0.7411\n",
            "Epoch 76/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1771 - accuracy: 0.7759\n",
            "Epoch 76: val_loss did not improve from 1.67894\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.1771 - accuracy: 0.7759 - val_loss: 1.6794 - val_accuracy: 0.7412\n",
            "Epoch 77/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1672 - accuracy: 0.7772\n",
            "Epoch 77: val_loss did not improve from 1.67894\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.1672 - accuracy: 0.7772 - val_loss: 1.6794 - val_accuracy: 0.7416\n",
            "Epoch 78/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1564 - accuracy: 0.7783\n",
            "Epoch 78: val_loss improved from 1.67894 to 1.67857, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.1564 - accuracy: 0.7783 - val_loss: 1.6786 - val_accuracy: 0.7416\n",
            "Epoch 79/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1452 - accuracy: 0.7797\n",
            "Epoch 79: val_loss did not improve from 1.67857\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.1452 - accuracy: 0.7797 - val_loss: 1.6807 - val_accuracy: 0.7413\n",
            "Epoch 80/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1341 - accuracy: 0.7810\n",
            "Epoch 80: val_loss improved from 1.67857 to 1.67808, saving model to ./model.h5\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.1341 - accuracy: 0.7810 - val_loss: 1.6781 - val_accuracy: 0.7419\n",
            "Epoch 81/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1243 - accuracy: 0.7823\n",
            "Epoch 81: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.1243 - accuracy: 0.7823 - val_loss: 1.6841 - val_accuracy: 0.7410\n",
            "Epoch 82/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1134 - accuracy: 0.7836\n",
            "Epoch 82: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.1134 - accuracy: 0.7836 - val_loss: 1.6791 - val_accuracy: 0.7422\n",
            "Epoch 83/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.1033 - accuracy: 0.7850\n",
            "Epoch 83: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.1033 - accuracy: 0.7850 - val_loss: 1.6806 - val_accuracy: 0.7425\n",
            "Epoch 84/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.0930 - accuracy: 0.7864\n",
            "Epoch 84: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.0930 - accuracy: 0.7864 - val_loss: 1.6825 - val_accuracy: 0.7426\n",
            "Epoch 85/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.0832 - accuracy: 0.7878\n",
            "Epoch 85: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.0832 - accuracy: 0.7878 - val_loss: 1.6826 - val_accuracy: 0.7422\n",
            "Epoch 86/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.0735 - accuracy: 0.7887\n",
            "Epoch 86: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.0735 - accuracy: 0.7887 - val_loss: 1.6843 - val_accuracy: 0.7427\n",
            "Epoch 87/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.0648 - accuracy: 0.7901\n",
            "Epoch 87: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.0648 - accuracy: 0.7901 - val_loss: 1.6851 - val_accuracy: 0.7422\n",
            "Epoch 88/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.0543 - accuracy: 0.7915\n",
            "Epoch 88: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.0543 - accuracy: 0.7915 - val_loss: 1.6880 - val_accuracy: 0.7423\n",
            "Epoch 89/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.0455 - accuracy: 0.7927\n",
            "Epoch 89: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.0455 - accuracy: 0.7927 - val_loss: 1.6882 - val_accuracy: 0.7428\n",
            "Epoch 90/500\n",
            "42/42 [==============================] - ETA: 0s - loss: 1.0357 - accuracy: 0.7942\n",
            "Epoch 90: val_loss did not improve from 1.67808\n",
            "42/42 [==============================] - 57s 1s/step - loss: 1.0357 - accuracy: 0.7942 - val_loss: 1.6912 - val_accuracy: 0.7424\n",
            "Epoch 90: early stopping\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    [x_tr, y_tr[:, :-1]],\n",
        "    y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)[:, 1:],\n",
        "    epochs=500,\n",
        "    callbacks=[es, save_model],\n",
        "    batch_size=1024,\n",
        "    validation_data=([x_val, y_val[:, :-1]],\n",
        "                     y_val.reshape(y_val.shape[0], y_val.shape[1], 1)[:, 1:]),\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b381b04",
      "metadata": {
        "id": "2b381b04",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model.load_weights('./model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57c30bff",
      "metadata": {
        "id": "57c30bff",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee459d5f",
      "metadata": {
        "id": "ee459d5f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Inference Models\n",
        "\n",
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs,\n",
        "                      state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim, ))\n",
        "decoder_state_input_c = Input(shape=(latent_dim, ))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "(decoder_outputs2, state_h2, state_c2) = decoder_lstm(dec_emb2,\n",
        "        initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model([decoder_inputs] + [decoder_hidden_state_input,\n",
        "                      decoder_state_input_h, decoder_state_input_c],\n",
        "                      [decoder_outputs2] + [state_h2, state_c2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd4f39a7",
      "metadata": {
        "id": "dd4f39a7"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "\n",
        "    # Encode the input as state vectors.\n",
        "    (e_out, e_h, e_c) = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Generate empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
        "                + [e_out, e_h, e_c], verbose=0)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        if sampled_token != 'eostok':\n",
        "            decoded_sentence += ' ' + sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find the stop word.\n",
        "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
        "            >= max_summary_len - 1:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1)\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        (e_h, e_c) = (h, c)\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d874896",
      "metadata": {
        "id": "8d874896"
      },
      "outputs": [],
      "source": [
        "# To convert sequence to text\n",
        "def seq2text(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if i != 0:\n",
        "            newString = newString + reverse_source_word_index[i] + ' '\n",
        "\n",
        "    return newString\n",
        "\n",
        "# To convert sequence to summary\n",
        "def seq2summary(input_seq):\n",
        "    newString = ''\n",
        "    for i in input_seq:\n",
        "        if (i != 0) and (i != target_word_index['sostok']) and (i != target_word_index['eostok']):\n",
        "            newString = newString + reverse_target_word_index[i] + ' '\n",
        "\n",
        "    return newString"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30bbbfd7",
      "metadata": {
        "id": "30bbbfd7"
      },
      "source": [
        "# Prediction summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4559955",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f4559955",
        "outputId": "a7993dda-544f-46f1-8532-54e2847e4e9d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Review: actress deepika padukone has denied reports that she is portraying amrita pritam in an upcoming biopic on poet ludhianvi which is being produced by sanjay leela bhansali deepika has currently signed only vishal bhardwaj s next production where she will be the role of gangster khan who was popularly known as sapna didi \n",
            "Original summary: start deepika denies starring as amrita in bhansali s film end \n",
            "Predicted summary:  start deepika to play in biopic on padmavati row report end\n",
            "\n",
            "Review: neelima azim while talking about being a single mother and sons shahid kapoor and ishaan khatter said i was a single mother and till today he shahid is taking care of ishaan and me in every way he took our lives forward in mumbai we started living better we had all the and comfort she added \n",
            "Original summary: start shahid takes care of ishaan me on being single mom end \n",
            "Predicted summary:  start my son taimur is a dream to be a on my burkha end\n",
            "\n",
            "Review: an etihad airways pilot passed away on wednesday while operating a cargo flight from abu dhabi to amsterdam after the captain became in the flight the first officer issued an emergency call and safely diverted the aircraft to kuwait the airline said in a statement the pilot was pronounced dead after receiving medical treatment the statement added \n",
            "Original summary: start pilot dies on board airways plane mid air end \n",
            "Predicted summary:  start pilot flyers to fly plane with emergency landing end\n",
            "\n",
            "Review: people in nda presidential candidate ram nath kovind s village in up have started celebrating his victory claiming that his win is a conclusion the people in said that he used to study in the light of an oil lamp kovind s pictures were waved outside the school he studied in as a child \n",
            "Original summary: start celebrations begin at ram nath kovind s village end \n",
            "Predicted summary:  start pm modi to be called by a dictator in uk end\n",
            "\n",
            "Review: several guest lecturers in bhopal got their heads shaved off on sunday in protest against the madhya pradesh government demanding regularisation of their services we don t have any option rather than our heads chief minister shivraj singh chouhan is not listening to our demands a guest lecturer who also got her head tonsured said \n",
            "Original summary: start guest their heads in protest in mp end \n",
            "Predicted summary:  start women wear skirts in protest against end\n",
            "\n",
            "Review: facebook coo sheryl sandberg at an event earlier this week while talking about sexual harassment at workplaces said we can t tolerate harvey weinstein like behaviour she also said it s about the people around them that know and don t do anything last week hollywood producer harvey weinstein was accused of sexually harassing actresses for decades \n",
            "Original summary: start can t tolerate weinstein like behaviour facebook coo end \n",
            "Predicted summary:  start i don t know how to be used to stop facebook google ceo end\n",
            "\n",
            "Review: the gujarat election commission has ordered a probe after a controversial video allegedly aimed at spreading communal hatred was circulated in poll bound gujarat on saturday this comes after the election commission received a complaint alleging that the viral video was creating a fear against the muslim community to polarise the ongoing political campaign \n",
            "Original summary: start gujarat poll panel orders probe in controversial viral video end \n",
            "Predicted summary:  start ec orders removal of using fake news on facebook s end\n",
            "\n",
            "Review: facebook has suspended canadian firm claiming it may be affiliated with cambridge analytica and improperly received its users data in response to facebook s action said that it has never been and is not a part of cambridge analytica earlier it was revealed that cambridge analytica exploited the data of facebook users to meddle in the us elections \n",
            "Original summary: start facebook suspends canadian firm amid data scandal probe end \n",
            "Predicted summary:  start facebook suspends facebook over fake news data scandal end\n",
            "\n",
            "Review: former president of iraq and its first non arab head of state has passed away aged 83 was a veteran of the kurdish struggle for an independent state and founded the patriotic union of kurdistan in 1975 he stepped down in 2014 two years after suffering a stroke that led him to seek medical treatment in germany \n",
            "Original summary: start iraq s first non arab president passes away end \n",
            "Predicted summary:  start iraq s 1st state citizen to be held in uk end\n",
            "\n",
            "Review: american researchers have proposed a previously unrecognised fluid filled space inside and between tissues as a new organ called earlier understood as packed barrier like walls researchers described the fluid organ as shock absorbers for tissues the new structures may be important in cancer study and functioning of tissues and organs they said \n",
            "Original summary: start us scientists propose new organ in human body end \n",
            "Predicted summary:  start scientists propose new type of human system end\n",
            "\n",
            "Review: philippine president rodrigo duterte has said that chinese president xi jinping had assured to protect him from moves that could result in his removal from office we will be there if you need us we will not allow the philippines to go to the dogs duterte added quoting jinping he further called the chinese president s assurance very encouraging \n",
            "Original summary: start chinese prez xi won t allow my philippine prez end \n",
            "Predicted summary:  start trump slams pm modi over remarks on chinese end\n",
            "\n",
            "Review: a man found a pearl which experts claim could be worth up to ã¢ââ¹2 8 lakh while having an pan costing over ã¢ââ¹1 000 at a new york restaurant the 66 year old said he felt a small object rolling around his mouth while having the dish that was by far the best pan i ever ordered he added \n",
            "Original summary: start us man finds worth ã¢ââ¹2 8 lakh while eating ã¢ââ¹1 000 dish end \n",
            "Predicted summary:  start man of us woman s body to pay ã¢ââ¹1 crore from end\n",
            "\n",
            "Review: the police in uttar pradesh have been directed to store ganga water for at police stations as well as to replace that get damaged on the way the directive was issued by the state s dgp and principal secretary home the bring ganga water from rishikesh and haridwar and travel with the on foot \n",
            "Original summary: start up police stations to store ganga water for end \n",
            "Predicted summary:  start gurugram police stations to be built by a year end\n",
            "\n",
            "Review: indian all rounder ravindra jadeja thanked pm modi on social media for inspiring indians to stay fit jadeja s post featured two photos one of him riding a bicycle in west indies while the other in which pm modi was seen riding a bicycle gifted to him by netherlands prime minister mark rutte \n",
            "Original summary: start jadeja thanks pm modi for indians to stay fit end \n",
            "Predicted summary:  start pm modi s speech at rashtrapati bhavan end\n",
            "\n",
            "Review: tesla co founder and ceo elon musk in an interview said that he may buy general motors plants in north america if they re going to sell a plant or not use it this comes after general motors in november announced it may shut down seven plants worldwide tesla had previously bought its california plant for 42 million in 2010 \n",
            "Original summary: start tesla could buy general motors plants ceo elon musk end \n",
            "Predicted summary:  start tesla to buy tesla cars in india s ceo end\n",
            "\n",
            "Review: finance minister arun jaitley on thursday said the union cabinet was briefed about the situation in jammu and kashmir where the governor has dissolved the state assembly though jammu and kashmir was not on the agenda of the meeting members of the cabinet were briefed about the situation he added the governor abruptly dissolved the state assembly on wednesday night \n",
            "Original summary: start cabinet was on jammu kashmir arun jaitley end \n",
            "Predicted summary:  start j k govt to take part in j k as j k cm end\n",
            "\n",
            "Review: based e scooter startup has paused its services in switzerland after it received reports of its e scooters abruptly halting mid ride throwing off and injuring users in a message to users said it s investigating if a software update could be causing the problem the startup added they will do a thorough security and quality check on their devices \n",
            "Original summary: start startup halts service as users fall from e scooters mid ride end \n",
            "Predicted summary:  start startup lets users control of its own company end\n",
            "\n",
            "Review: samajwadi party chief akhilesh yadav on friday said that he is getting severely abused on social media adding that he filed a complaint with the police but no action was taken but in similar cases with bjp leaders instant action is taken and the culprit is thrown in jail the former uttar pradesh cm said \n",
            "Original summary: start i m abused online police takes no action akhilesh end \n",
            "Predicted summary:  start i m not a fool to be tej pratap on his birthday end\n",
            "\n",
            "Review: the son of former sri lankan president mahinda rajapaksa was arrested along with two other lawmakers for staging anti india protests outside the indian consulate in hambantota they were protesting against alleged government plans to lease an airport to an indian company however the sri lankan government has denied that it is planning to sell off the airport \n",
            "Original summary: start son of ex sri lanka prez held for anti india protest end \n",
            "Predicted summary:  start ex pak prez mugabe s wife after being hit by india end\n",
            "\n",
            "Review: the world bank s ease of doing business report ranks 190 economies based on ten indicators these a business dealing with construction permits getting electricity registering property paying taxes enforcing contracts trading across borders resolving insolvency protecting minority investors and getting credit india which ranked 100 saw improvement in 6 indicators \n",
            "Original summary: start what are the used to rate ease of doing business end \n",
            "Predicted summary:  start india s largest economy to be in india end\n",
            "\n",
            "Review: two new posters of parineeti chopra and arjun kapoor starrer namaste england have been unveiled parineeti and arjun who worked together for the first time in arjun s debut film ishaqzaade which released in the year 2012 will be after six years with namaste england directed by vipul amrutlal shah the film is scheduled to release on october 19 \n",
            "Original summary: start new posters of parineeti arjun starrer namaste england out end \n",
            "Predicted summary:  start new poster of parineeti s namaste england released end\n",
            "\n",
            "Review: the rajasthan government on monday announced 1 reservations for five gujjar and under the most backward classes category this comes after gujjar leaders threatened to launch protests during pm narendra modi s upcoming jaipur visit the were seeking 5 reservation within obc quota to not violate the 50 supreme court mandated cap \n",
            "Original summary: start rajasthan approves 1 reservation for other end \n",
            "Predicted summary:  start rajasthan to get up reservation for free wifi in end\n",
            "\n",
            "Review: technology giant google has launched app that lets users control and monitor their data usage in real time the app which is available to all android users lets users turn on the data tab to block background data usage for the same purpose the app also helps users save data by notifying them about nearby public wi fi connections \n",
            "Original summary: start google launches app to let users control data end \n",
            "Predicted summary:  start google launches app that can users users end\n",
            "\n",
            "Review: tesla co founder and ceo elon musk on thursday shared the pictures of two drawings he made on a tesla car s touchscreen the drawings included a unicorn and a portrait tesla has launched an update to its car software that lets people create drawings on the car s touchscreen by tapping the letter t three times on the screen \n",
            "Original summary: start elon musk shares he made on tesla car s touchscreen end \n",
            "Predicted summary:  start elon musk shares tesla s look from tesla car end\n",
            "\n",
            "Review: the first draft of the national register of citizens nrc for assam recognised 1 9 crore people as indian citizens the verification process for the rest 1 39 crore applicants is still on registrar general of india said on sunday assam which has witnessed an influx of migrants from bangladesh became the only state to have nrc in 1951 \n",
            "Original summary: start assam recognises 1 9 cr legal citizens in first nrc draft end \n",
            "Predicted summary:  start nepal to get 1 000 crore in india s last new cabinet end\n",
            "\n",
            "Review: the government will launch a nationwide toll free helpline within a week to provide support related to cashless transactions reports said the cash mukt bharat abhiyan helpline will help people find suitable options depending on whether they have a smartphone or not earlier this week the government launched a new television channel and a website to promote digital payments \n",
            "Original summary: start government to launch helpline number for e payments end \n",
            "Predicted summary:  start govt to launch e vehicles to get tablets in india end\n",
            "\n",
            "Review: kerala chief minister pinarayi vijayan on saturday announced an ex gratia of ã¢ââ¹10 lakh for the of those who died in rain related incidents caused by the cyclone ockhi free treatment and food will be provided for the victims admitted to hospitals he added he also announced free ration for families in coastal areas of kerala for a week \n",
            "Original summary: start kerala cm announces ã¢ââ¹10l ex gratia for cyclone ockhi victims end \n",
            "Predicted summary:  start up cm announces ã¢ââ¹5l for liquor shops in bihar end\n",
            "\n",
            "Review: india wicketkeeper batsman ms dhoni advised chinaman bowler kuldeep yadav to bowl from round the wicket to new zealand s trent boult on the last delivery of the 38th over ye band se round the wicket daal hai dhoni told kuldeep boult edged kuldeep s round the wicket googly and was caught out by rohit sharma at slips \n",
            "Original summary: start band dhoni tells kuldeep how to dismiss end \n",
            "Predicted summary:  start dhoni to play county cricket in cricket end\n",
            "\n",
            "Review: us president donald trump on friday said that the country would impose tough sanctions on iran s revolutionary guard corps accusing the organisation of supporting terrorism trump called on the us treasury department to apply sanctions to the organisations officials agents and affiliates he further urged us allies to take strong actions to curb iran s continued dangerous behaviour \n",
            "Original summary: start us imposes sanctions on iran s guard end \n",
            "Predicted summary:  start us imposes sanctions on north korea end\n",
            "\n",
            "Review: girl who got married at the age of eight and was sent to her husband s house before completing her class 10 has cracked neet to pursue her dream of becoming a doctor while child marriage is illegal in india and is still common in rural areas the 20 year old girl s husband helped her to continue her education \n",
            "Original summary: start girl who got married at the age of 8 cracks neet end \n",
            "Predicted summary:  start girl who took 8 wickets in a day of his name on his name end\n",
            "\n",
            "Review: indian captain virat kohli s instagram account which has over 19 8 million followers so far was named the most engaged account as part of the 2017 instagram awards in india the award was given to kohli as his account saw the biggest number of engagements including likes and comments on the content that he shared in the past year \n",
            "Original summary: start kohli wins instagram s most engaged account award end \n",
            "Predicted summary:  start kohli s pay tribute to marvel s most liked instagram end\n",
            "\n",
            "Review: hollywood actor sylvester stallone took to social media to share the first poster of his upcoming film rambo v something is coming this way wrote the actor while sharing the poster rambo v the fifth instalment in the rambo film series is set to release in late 2019 \n",
            "Original summary: start sylvester stallone shares first poster of his film rambo v end \n",
            "Predicted summary:  start sylvester stallone shares first look from rambo remake end\n",
            "\n",
            "Review: a in brazilian city rio de janeiro where nearly a million african slaves are believed to have landed was declared a unesco world heritage site over the weekend the constructed in became the largest entry point for african slaves in brazil its remains were discovered in 2011 during pre olympic renovations \n",
            "Original summary: start where african landed gets world heritage status end \n",
            "Predicted summary:  start railway station lit up to win for world cup end\n",
            "\n",
            "Review: swedish fashion retailer h m apologised for an advertisement showing a black child in a bearing the words coolest monkey in the jungle while also recalling the controversial garment from their stores this comes after people slammed h m calling the advertisement racist and unacceptable this image has been removed from all h m channels said a spokesperson \n",
            "Original summary: start h m recalls sale of racist apologises for their ad end \n",
            "Predicted summary:  start uk s ad slammed for racist porn on twitter end\n",
            "\n",
            "Review: the supreme court on thursday observed when during elections politicians can go among the public to seek votes why can t people come near to their offices after polls to protest the court was hearing a plea by advocate prashant bhushan challenging the centre s decision to permanently impose section 144 of crpc in central delhi banning protests by people \n",
            "Original summary: start if seek vote can t people protest near their homes sc end \n",
            "Predicted summary:  start no immediate ban on liquor ban on highways sc end\n",
            "\n",
            "Review: siddaramaiah on thursday accepted a nine member committee s design for the proposed official state flag the committee was set up last year to examine the legal feasibility of an official state flag after demands by pro kannada activists if the centre approves the proposal karnataka will be the second state to have a separate flag after jammu and kashmir \n",
            "Original summary: start karnataka government unveils official state flag end \n",
            "Predicted summary:  start karnataka govt to launch new political entity end\n",
            "\n",
            "Review: commerce major amazon on wednesday began testing its self driving fully electric robot amazon to deliver packages to users in county in us washington the six robot strong fleet each the size of a small will deliver packages during daylight hours from monday to friday each robot will initially be accompanied by an employee to make package deliveries to users \n",
            "Original summary: start amazon tests self driving electric robots for delivery in us end \n",
            "Predicted summary:  start amazon patents autonomous car powered robots end\n",
            "\n",
            "Review: a 20 year old dalit woman allegedly gangraped by two brothers for several months in madhya pradesh s satna took her foetus to the superintendent of police on wednesday police booked the accused their two accomplices and the nurse who performed the forced abortion the woman alleged that a police station had earlier refused to take action against the accused \n",
            "Original summary: start woman gangrape takes foetus to mp police end \n",
            "Predicted summary:  start woman gangraped for opposing ruckus at mp end\n",
            "\n",
            "Review: french presidential candidate marine le pen on sunday said that her plans to withdraw the country from the eurozone wouldn t be chaos and she would seek well prepared talks with other european union countries the eurozone is a serious to job creation because it triggered a loss in competitiveness for the french economy le pen added \n",
            "Original summary: start french exit from won t be chaos prez candidate end \n",
            "Predicted summary:  start french prez macron is a new air india end\n",
            "\n",
            "Review: a photograph of actor ranbir kapoor from the sets of the upcoming biopic on actor sanjay dutt has been shared online ranbir can be seen with long hair and is dressed in a shirt with a and black trousers in the picture ranbir has reportedly gained 13 kilograms to portray dutt in the film \n",
            "Original summary: start pic of ranbir from sets of sanjay dutt biopic shared online end \n",
            "Predicted summary:  start pic of salman khan s look from sets of upcoming film end\n",
            "\n",
            "Review: abdullah saleh was killed on monday by houthi rebel fighters according to reports saleh s death was announced by houthi controlled interior ministry as well as saudi owned al quoting saleh s party sources this comes a day after the former president formally renounced his alliance with the iran backed rebel group and re aligned his forces with saudi arabia \n",
            "Original summary: start former president killed by houthi rebels end \n",
            "Predicted summary:  start former maldives prez passes away aged 83 end\n",
            "\n",
            "Review: elon musk led tunnelling startup the boring company has shared a video showing its latest tunnel digging machine being operated by an xbox controller in the video the head of the machine is seen and rolling while placing the tunnel sections on the sides entirely controlled by the remote the startup is currently working with 3 different boring machines \n",
            "Original summary: start boring company uses remote to control digging machine end \n",
            "Predicted summary:  start spacex tesla s boring company to launch drone on its first ever end\n",
            "\n",
            "Review: american mattel has unveiled the sequel of card game for the first time since it was invented in 1971 dos uses a similar system of numbered and coloured cards but the players now have two card to choose from for playing cards and win after earning enough points \n",
            "Original summary: start unveiled as sequel to card game end \n",
            "Predicted summary:  start new fashion brand gucci s new opera end\n",
            "\n",
            "Review: calling on the international community to oppose the us recent trade moves chinese foreign minister wang yi said if the us believes it could benefit from protectionism it is making a mistake china also launched a world trade organisation challenge against the us proposal to impose tariffs on chinese imports over alleged theft of intellectual property and technology \n",
            "Original summary: start us making a mistake by imposing tariffs on imports china end \n",
            "Predicted summary:  start us warns us defence secy tillerson s trade war end\n",
            "\n",
            "Review: comedian amy took to instagram to reveal that she got married to her boyfriend chef chris fischer in a secret wedding ceremony the wedding reportedly took place in on tuesday the wedding was attended by celebrities including jennifer lawrence jake jennifer aniston larry david and david spade \n",
            "Original summary: start amy gets married to boyfriend in secret ceremony end \n",
            "Predicted summary:  start singer sapna s wedding gown with girlfriend end\n",
            "\n",
            "Review: the supreme court on thursday approved bcci s draft constitution the one state one vote policy recommended by the lodha panel each state was allowed only one association as member while those with multiple associations were allowed membership on rotational basis subsequently the court has given full membership status to mumbai saurashtra and vidarbha associations among others \n",
            "Original summary: start supreme court dumps bcci s one state one vote policy end \n",
            "Predicted summary:  start sc stays bcci s order on bcci s decision to end\n",
            "\n",
            "Review: the world premiere of ali fazal and starrer victoria and abdul was held at the 74th venice international film festival directed by stephen the biographical drama revolves around the real life relationship between queen victoria and her indian servant abdul karim the film will also be screened at the toronto international film festival \n",
            "Original summary: start ali s victoria and abdul at venice film fest end \n",
            "Predicted summary:  start first song of akshay kumar starrer released end\n",
            "\n",
            "Review: nasa s 28 year old hubble space telescope has resumed its operations after 22 days the agency said on saturday the telescope had suffered a backup malfunction on october 5 which caused it to show the rate of rotation far in excess of the actual rates nasa was able to and hubble by switching it between different operational modes \n",
            "Original summary: start nasa s 28 yr old telescope active again after 22 days end \n",
            "Predicted summary:  start nasa telescope to earth s first ever in the milky way end\n",
            "\n",
            "Review: restaurant table reservation startup has raised 5 85 million ã¢ââ¹41 crore in its latest funding round led by investment trust and early stage investment firm beenext cricketer yuvraj singh who earlier invested an undisclosed sum is the brand ambassador of the gurugram based startup currently operational in 11 indian cities is also setting up operations in dubai \n",
            "Original summary: start yuvraj singh backed table reservation startup raises ã¢ââ¹41 cr end \n",
            "Predicted summary:  start yuvraj singh backed startup raises crore end\n",
            "\n",
            "Review: chief justice of india js khehar on saturday said nowadays have become a mere piece of paper for this political parties have to be made accountable he further said none of the political parties indicated any link between electoral reforms and constitutional goal of ensuring economic social justice to the marginalised section \n",
            "Original summary: start parties must be held accountable for promises end \n",
            "Predicted summary:  start no longer a crime in delhi smog ex delhi chief end\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-655c34c3-0223-4086-9301-6bcbcd2a9f50\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Actual</th>\n",
              "      <th>Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>start deepika denies starring as amrita in bha...</td>\n",
              "      <td>start deepika to play in biopic on padmavati ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>start shahid takes care of ishaan me on being ...</td>\n",
              "      <td>start my son taimur is a dream to be a on my ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>start pilot dies on board airways plane mid ai...</td>\n",
              "      <td>start pilot flyers to fly plane with emergenc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>start celebrations begin at ram nath kovind s ...</td>\n",
              "      <td>start pm modi to be called by a dictator in u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>start guest their heads in protest in mp end</td>\n",
              "      <td>start women wear skirts in protest against end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>start can t tolerate weinstein like behaviour ...</td>\n",
              "      <td>start i don t know how to be used to stop fac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>start gujarat poll panel orders probe in contr...</td>\n",
              "      <td>start ec orders removal of using fake news on...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>start facebook suspends canadian firm amid dat...</td>\n",
              "      <td>start facebook suspends facebook over fake ne...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>start iraq s first non arab president passes a...</td>\n",
              "      <td>start iraq s 1st state citizen to be held in ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>start us scientists propose new organ in human...</td>\n",
              "      <td>start scientists propose new type of human sy...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-655c34c3-0223-4086-9301-6bcbcd2a9f50')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-655c34c3-0223-4086-9301-6bcbcd2a9f50 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-655c34c3-0223-4086-9301-6bcbcd2a9f50');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              Actual  \\\n",
              "0  start deepika denies starring as amrita in bha...   \n",
              "1  start shahid takes care of ishaan me on being ...   \n",
              "2  start pilot dies on board airways plane mid ai...   \n",
              "3  start celebrations begin at ram nath kovind s ...   \n",
              "4      start guest their heads in protest in mp end    \n",
              "5  start can t tolerate weinstein like behaviour ...   \n",
              "6  start gujarat poll panel orders probe in contr...   \n",
              "7  start facebook suspends canadian firm amid dat...   \n",
              "8  start iraq s first non arab president passes a...   \n",
              "9  start us scientists propose new organ in human...   \n",
              "\n",
              "                                           Predicted  \n",
              "0   start deepika to play in biopic on padmavati ...  \n",
              "1   start my son taimur is a dream to be a on my ...  \n",
              "2   start pilot flyers to fly plane with emergenc...  \n",
              "3   start pm modi to be called by a dictator in u...  \n",
              "4     start women wear skirts in protest against end  \n",
              "5   start i don t know how to be used to stop fac...  \n",
              "6   start ec orders removal of using fake news on...  \n",
              "7   start facebook suspends facebook over fake ne...  \n",
              "8   start iraq s 1st state citizen to be held in ...  \n",
              "9   start scientists propose new type of human sy...  "
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "actual = []\n",
        "predicted = []\n",
        "for i in range(0, 50):\n",
        "    print ('Review:', seq2text(x_tr[i]))\n",
        "\n",
        "    actual.append(seq2summary(y_tr[i]))\n",
        "    print ('Original summary:', actual[-1])\n",
        "\n",
        "    predicted.append(decode_sequence(x_tr[i].reshape(1, config['max_text_len'])))\n",
        "    print ('Predicted summary:', predicted[-1])\n",
        "    print()\n",
        "\n",
        "prediction_df = pd.DataFrame({'Actual':actual, 'Predicted':predicted})\n",
        "prediction_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "358fdeb1",
      "metadata": {
        "id": "358fdeb1"
      },
      "source": [
        "---\n",
        "# Rouge score\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0dd5839",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "e0dd5839",
        "outputId": "0ee6d48f-6ca7-4b80-a894-b95dd23e2933"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7ad0a415-23de-473b-bc47-9394bff7e9e3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Actual</th>\n",
              "      <th>Predicted</th>\n",
              "      <th>rouge_pr</th>\n",
              "      <th>rouge_rc</th>\n",
              "      <th>rouge_f</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>start deepika denies starring as amrita in bha...</td>\n",
              "      <td>start deepika to play in biopic on padmavati ...</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>start shahid takes care of ishaan me on being ...</td>\n",
              "      <td>start my son taimur is a dream to be a on my ...</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.307692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>start pilot dies on board airways plane mid ai...</td>\n",
              "      <td>start pilot flyers to fly plane with emergenc...</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>start celebrations begin at ram nath kovind s ...</td>\n",
              "      <td>start pm modi to be called by a dictator in u...</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>start guest their heads in protest in mp end</td>\n",
              "      <td>start women wear skirts in protest against end</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.470588</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>start can t tolerate weinstein like behaviour ...</td>\n",
              "      <td>start i don t know how to be used to stop fac...</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>start gujarat poll panel orders probe in contr...</td>\n",
              "      <td>start ec orders removal of using fake news on...</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.272727</td>\n",
              "      <td>0.260870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>start facebook suspends canadian firm amid dat...</td>\n",
              "      <td>start facebook suspends facebook over fake ne...</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>start iraq s first non arab president passes a...</td>\n",
              "      <td>start iraq s 1st state citizen to be held in ...</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.363636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>start us scientists propose new organ in human...</td>\n",
              "      <td>start scientists propose new type of human sy...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.631579</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ad0a415-23de-473b-bc47-9394bff7e9e3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7ad0a415-23de-473b-bc47-9394bff7e9e3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7ad0a415-23de-473b-bc47-9394bff7e9e3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              Actual  \\\n",
              "0  start deepika denies starring as amrita in bha...   \n",
              "1  start shahid takes care of ishaan me on being ...   \n",
              "2  start pilot dies on board airways plane mid ai...   \n",
              "3  start celebrations begin at ram nath kovind s ...   \n",
              "4      start guest their heads in protest in mp end    \n",
              "5  start can t tolerate weinstein like behaviour ...   \n",
              "6  start gujarat poll panel orders probe in contr...   \n",
              "7  start facebook suspends canadian firm amid dat...   \n",
              "8  start iraq s first non arab president passes a...   \n",
              "9  start us scientists propose new organ in human...   \n",
              "\n",
              "                                           Predicted  rouge_pr  rouge_rc  \\\n",
              "0   start deepika to play in biopic on padmavati ...  0.363636  0.363636   \n",
              "1   start my son taimur is a dream to be a on my ...  0.285714  0.333333   \n",
              "2   start pilot flyers to fly plane with emergenc...  0.400000  0.400000   \n",
              "3   start pm modi to be called by a dictator in u...  0.166667  0.200000   \n",
              "4     start women wear skirts in protest against end  0.500000  0.444444   \n",
              "5   start i don t know how to be used to stop fac...  0.266667  0.400000   \n",
              "6   start ec orders removal of using fake news on...  0.250000  0.272727   \n",
              "7   start facebook suspends facebook over fake ne...  0.600000  0.600000   \n",
              "8   start iraq s 1st state citizen to be held in ...  0.333333  0.400000   \n",
              "9   start scientists propose new type of human sy...  0.666667  0.600000   \n",
              "\n",
              "    rouge_f  \n",
              "0  0.363636  \n",
              "1  0.307692  \n",
              "2  0.400000  \n",
              "3  0.181818  \n",
              "4  0.470588  \n",
              "5  0.320000  \n",
              "6  0.260870  \n",
              "7  0.600000  \n",
              "8  0.363636  \n",
              "9  0.631579  "
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "r_score = []\n",
        "rouge_pr = []\n",
        "rouge_rc = []\n",
        "rouge_f = []\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "for indx, data in prediction_df.iterrows():\n",
        "    r_score = scorer.score(data.Actual, data.Predicted)\n",
        "    pr = list(r_score['rouge1'])[0]\n",
        "    rc = list(r_score['rouge1'])[1]\n",
        "    fmeas = list(r_score['rouge1'])[2]\n",
        "\n",
        "    rouge_pr.append(pr)\n",
        "    rouge_rc.append(rc)\n",
        "    rouge_f.append(fmeas)\n",
        "\n",
        "prediction_df['rouge_pr'] = rouge_pr\n",
        "prediction_df['rouge_rc'] = rouge_rc\n",
        "prediction_df['rouge_f'] = rouge_f\n",
        "prediction_df.head(10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 28883.151012,
      "end_time": "2022-02-18T09:02:05.753059",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-02-18T01:00:42.602047",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}